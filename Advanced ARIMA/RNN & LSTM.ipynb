{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 22:57:34.373971: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-17 22:57:35.037350: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-17 22:57:35.037405: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-17 22:57:35.037412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import calendar\n",
    "import collections.abc\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 22:57:36.127049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-17 22:57:36.178875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-17 22:57:36.179039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col='date')\n",
    "train.index = pd.to_datetime(train.index)\n",
    "train = pd.pivot_table(train, values='sales', index=\"date\", columns=['store_nbr', 'family'], aggfunc=np.sum)\n",
    "train.columns = [f\"{a[0]}_{a[1].replace('/','_')}\" for a in train.columns]\n",
    "exog = pickle.load(open('exog.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dcoilwtico</th>\n",
       "      <th>National Holiday</th>\n",
       "      <th>Cotopaxi</th>\n",
       "      <th>Imbabura</th>\n",
       "      <th>Santa Elena</th>\n",
       "      <th>Santo Domingo de los Tsachilas</th>\n",
       "      <th>Ambato</th>\n",
       "      <th>Cayambe</th>\n",
       "      <th>Cuenca</th>\n",
       "      <th>El Carmen</th>\n",
       "      <th>...</th>\n",
       "      <th>Loja</th>\n",
       "      <th>Machala</th>\n",
       "      <th>Manta</th>\n",
       "      <th>Puyo</th>\n",
       "      <th>Quevedo</th>\n",
       "      <th>Quito</th>\n",
       "      <th>Riobamba</th>\n",
       "      <th>Salinas</th>\n",
       "      <th>Santo Domingo</th>\n",
       "      <th>Payday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>93.205556</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>93.140000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>92.970000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>93.120000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>93.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-11</th>\n",
       "      <td>48.810000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-12</th>\n",
       "      <td>48.810000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-13</th>\n",
       "      <td>47.590000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-14</th>\n",
       "      <td>47.590000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-15</th>\n",
       "      <td>47.570000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1684 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dcoilwtico  National Holiday  Cotopaxi  Imbabura  Santa Elena  \\\n",
       "date                                                                        \n",
       "2013-01-01   93.205556                 1       0.0       0.0          0.0   \n",
       "2013-01-02   93.140000                 0       0.0       0.0          0.0   \n",
       "2013-01-03   92.970000                 0       0.0       0.0          0.0   \n",
       "2013-01-04   93.120000                 0       0.0       0.0          0.0   \n",
       "2013-01-05   93.120000                 1       0.0       0.0          0.0   \n",
       "...                ...               ...       ...       ...          ...   \n",
       "2017-08-11   48.810000                 1       0.0       0.0          0.0   \n",
       "2017-08-12   48.810000                 0       0.0       0.0          0.0   \n",
       "2017-08-13   47.590000                 0       0.0       0.0          0.0   \n",
       "2017-08-14   47.590000                 0       0.0       0.0          0.0   \n",
       "2017-08-15   47.570000                 0       0.0       0.0          0.0   \n",
       "\n",
       "            Santo Domingo de los Tsachilas  Ambato  Cayambe  Cuenca  \\\n",
       "date                                                                  \n",
       "2013-01-01                             0.0     0.0      0.0     0.0   \n",
       "2013-01-02                             0.0     0.0      0.0     0.0   \n",
       "2013-01-03                             0.0     0.0      0.0     0.0   \n",
       "2013-01-04                             0.0     0.0      0.0     0.0   \n",
       "2013-01-05                             0.0     0.0      0.0     0.0   \n",
       "...                                    ...     ...      ...     ...   \n",
       "2017-08-11                             0.0     0.0      0.0     0.0   \n",
       "2017-08-12                             0.0     0.0      0.0     0.0   \n",
       "2017-08-13                             0.0     0.0      0.0     0.0   \n",
       "2017-08-14                             0.0     0.0      0.0     0.0   \n",
       "2017-08-15                             0.0     0.0      0.0     0.0   \n",
       "\n",
       "            El Carmen  ...  Loja  Machala  Manta  Puyo  Quevedo  Quito  \\\n",
       "date                   ...                                               \n",
       "2013-01-01        0.0  ...   0.0      0.0    0.0   0.0      0.0    0.0   \n",
       "2013-01-02        0.0  ...   0.0      0.0    0.0   0.0      0.0    0.0   \n",
       "2013-01-03        0.0  ...   0.0      0.0    0.0   0.0      0.0    0.0   \n",
       "2013-01-04        0.0  ...   0.0      0.0    0.0   0.0      0.0    0.0   \n",
       "2013-01-05        0.0  ...   0.0      0.0    0.0   0.0      0.0    0.0   \n",
       "...               ...  ...   ...      ...    ...   ...      ...    ...   \n",
       "2017-08-11        0.0  ...   0.0      0.0    0.0   0.0      0.0    0.0   \n",
       "2017-08-12        0.0  ...   0.0      0.0    0.0   0.0      0.0    0.0   \n",
       "2017-08-13        0.0  ...   0.0      0.0    0.0   0.0      0.0    0.0   \n",
       "2017-08-14        0.0  ...   0.0      0.0    0.0   0.0      0.0    0.0   \n",
       "2017-08-15        0.0  ...   0.0      0.0    0.0   0.0      0.0    0.0   \n",
       "\n",
       "            Riobamba  Salinas  Santo Domingo  Payday  \n",
       "date                                                  \n",
       "2013-01-01       0.0      0.0            0.0       0  \n",
       "2013-01-02       0.0      0.0            0.0       0  \n",
       "2013-01-03       0.0      0.0            0.0       0  \n",
       "2013-01-04       0.0      0.0            0.0       0  \n",
       "2013-01-05       0.0      0.0            0.0       0  \n",
       "...              ...      ...            ...     ...  \n",
       "2017-08-11       0.0      0.0            0.0       0  \n",
       "2017-08-12       0.0      0.0            0.0       0  \n",
       "2017-08-13       0.0      0.0            0.0       0  \n",
       "2017-08-14       0.0      0.0            0.0       0  \n",
       "2017-08-15       1.0      0.0            0.0       1  \n",
       "\n",
       "[1684 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = np.empty((train.shape[0], train.shape[1], 16))\n",
    "for time_step in range(train.shape[0]):\n",
    "    for feature in range(train.shape[1]):\n",
    "        emp[time_step, feature] = [train.iloc[time_step+i, feature] if time_step+i<train.shape[0] else 0 for i in range(16)]\n",
    "Y = emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='loss', mode='min', patience=10, min_delta=0.05)\n",
    "X_train, X_test, y_train, y_test = train_test_split(exog, Y, test_size=0.10, random_state=5, shuffle=False)\n",
    "X_train = X_train.values.reshape(1, X_train.shape[0], X_train.shape[1])\n",
    "X_test = X_test.values.reshape(1, X_test.shape[0], X_test.shape[1])\n",
    "y_train = y_train.reshape(1, y_train.shape[0], y_train.shape[1], y_train.shape[2])\n",
    "y_test = y_test.reshape(1, y_test.shape[0], y_test.shape[1], y_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1515, 26)\n",
      "(1, 169, 26)\n",
      "(1, 1515, 1782, 16)\n",
      "(1, 169, 1782, 16)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_120 (SimpleRNN)  (None, None, 100)         12700     \n",
      "                                                                 \n",
      " simple_rnn_121 (SimpleRNN)  (None, None, 100)         20100     \n",
      "                                                                 \n",
      " simple_rnn_122 (SimpleRNN)  (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 16)                1616      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,516\n",
      "Trainable params: 54,516\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 14.5474\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 13.7668\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 13.0791\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 12.5006\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 12.1461\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 11.8989\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 11.7017\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 11.5400\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 11.4093\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 11.3010\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 11.2079\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 11.1258\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 11.0532\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.9893\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.9327\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.8813\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.8337\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.7889\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.7467\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.7069\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.6694\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.6340\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.6006\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.5691\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.5392\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.5109\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.4840\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.4584\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.4341\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.4108\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.3885\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.3671\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.3465\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.3266\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.3073\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.2888\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.2708\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.2533\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.2364\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.2200\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.2041\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.1886\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.1736\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.1590\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.1447\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.1309\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.1174\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.1042\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.0914\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.0789\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.0667\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.0548\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.0432\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.0318\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.0207\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.0099\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.9993\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.9890\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.9789\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.9691\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.9594\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.9500\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.9408\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.9318\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.9230\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.9143\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.9059\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8976\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8895\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8816\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8738\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8662\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8588\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8515\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8443\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8373\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8304\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8236\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8170\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8105\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.8041\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7979\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7917\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7857\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7798\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7739\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7682\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7626\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7571\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7517\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7464\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7411\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7360\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7309\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7259\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7211\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7162\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7115\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7069\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.7023\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.6978\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.6934\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.6890\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.6847\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 9.6805\n",
      "1/1 [==============================] - 0s 216ms/step\n"
     ]
    }
   ],
   "source": [
    "rnn = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(100, return_sequences=True, input_shape=[None, X_train.shape[2]]),\n",
    "    keras.layers.SimpleRNN(100, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(100),\n",
    "    keras.layers.Dense(16)\n",
    "])\n",
    "rnn.compile(loss=keras.losses.MeanSquaredLogarithmicError(), optimizer=tf.keras.optimizers.Adam())\n",
    "rnn.summary()\n",
    "rnn.fit(X_train, y_train, epochs=200, callbacks=[es])\n",
    "pred_rnn = np.round(abs(rnn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8., 8., 9., 2., 1., 1., 9., 7., 2., 8., 7., 8., 8., 8., 8., 8.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pred_rnn.shape)\n",
    "pred_rnn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
